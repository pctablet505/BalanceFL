{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bed50c29",
   "metadata": {},
   "source": [
    "# DeepFed: Time-Series Intrusion Detection System\n",
    "\n",
    "This notebook implements a comprehensive DeepFed model for intrusion detection using the Edge-IIoTset dataset. The system includes:\n",
    "\n",
    "- **Dataset**: Edge-IIoTset - Cyber Security Dataset of IoT & IIoT\n",
    "- **Model**: GRU + CNN (Time-Series Architecture) \n",
    "- **Features**: Multi-class attack type classification, efficient HDF5 caching, interactive data exploration\n",
    "\n",
    "## Section 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e74f90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All required libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import zipfile\n",
    "import subprocess\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use non-interactive backend for notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Use Keras 3 (not tensorflow.keras)\n",
    "import keras\n",
    "from keras import layers, callbacks\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler, OrdinalEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check for required packages\n",
    "try:\n",
    "    import tables  # For HDF5 support\n",
    "except ImportError:\n",
    "    print(\"ERROR: pytables is required for HDF5 export. Please install with `pip install tables`.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"✓ All required libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39437406",
   "metadata": {},
   "source": [
    "## Section 2: Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "be663ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration set and directories created!\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DATASET_NAME = \"mohamedamineferrag/edgeiiotset-cyber-security-dataset-of-iot-iiot\"\n",
    "DATA_DIR = Path(\"./data/edge_iiot\")\n",
    "MODELS_DIR = Path(\"./models/deepfed\")\n",
    "VISUALIZATIONS_DIR = Path(\"./visualizations\")\n",
    "CACHE_DIR = Path(\"./cache\")\n",
    "PREPROCESSED_DIR = CACHE_DIR / \"preprocessed\"\n",
    "HDF5_DATASET = PREPROCESSED_DIR / \"dataset.h5\"\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "MAX_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-3\n",
    "RANDOM_STATE = 42\n",
    "SEQUENCE_LENGTH = 128  # Time steps for time-series sequences (set None for dynamic windows)\n",
    "WINDOW_STRIDE = 1\n",
    "VALIDATION_SPLIT = 0.2\n",
    "SAMPLE_SIZE = 20000  # Use a small sample for debugging; set to None for full dataset\n",
    "MAX_SEQUENCES = 20000  # Downsample generated sequences for faster experimentation (set None to disable)\n",
    "USE_MULTICLASS = True  # Use multi-class attack type classification\n",
    "USE_CACHED_DATA = True  # Reuse cached binary dataset once generated\n",
    "\n",
    "# Model hyperparameters\n",
    "GRU_UNITS = 128\n",
    "GRU_DROPOUT = 0.3\n",
    "CNN_FILTERS = 128\n",
    "CNN_KERNEL_SIZE = 3\n",
    "MLP_UNITS = 256\n",
    "MLP_DROPOUT = 0.4\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(RANDOM_STATE)\n",
    "keras.utils.set_random_seed(RANDOM_STATE)\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [DATA_DIR, MODELS_DIR, VISUALIZATIONS_DIR, CACHE_DIR, PREPROCESSED_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"✓ Configuration set and directories created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b310a39",
   "metadata": {},
   "source": [
    "## Section 3: Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff0db316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset already exists!\n"
     ]
    }
   ],
   "source": [
    "def download_dataset():\n",
    "    \"\"\"Download Edge-IIoTset dataset from Kaggle\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"DOWNLOADING EDGE-IIOTSET DATASET FROM KAGGLE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Setup Kaggle credentials from Colab secrets\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        print(\"✓ Running in Google Colab - using secrets\")\n",
    "        \n",
    "        # Get credentials from Colab secrets\n",
    "        kaggle_username = userdata.get('KAGGLE_USERNAME')\n",
    "        kaggle_key = userdata.get('KAGGLE_KEY')\n",
    "        \n",
    "        if not kaggle_username or not kaggle_key:\n",
    "            raise ValueError(\"KAGGLE_USERNAME and KAGGLE_KEY must be set in Colab secrets\")\n",
    "        \n",
    "        # Set environment variables for Kaggle API\n",
    "        os.environ['KAGGLE_USERNAME'] = kaggle_username\n",
    "        os.environ['KAGGLE_KEY'] = kaggle_key\n",
    "        \n",
    "        print(f\"  • Username: {kaggle_username}\")\n",
    "        print(f\"  • API Key: {'*' * len(kaggle_key)}\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"✓ Not running in Colab - using default kaggle.json authentication\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error setting up Kaggle credentials: {e}\")\n",
    "        print(\"\\nPlease add these secrets in Colab:\")\n",
    "        print(\"  1. Click the key icon (🔑) in the left sidebar\")\n",
    "        print(\"  2. Add secret: KAGGLE_USERNAME\")\n",
    "        print(\"  3. Add secret: KAGGLE_KEY\")\n",
    "        print(\"\\nGet your credentials from: https://www.kaggle.com/settings/account\")\n",
    "        raise\n",
    "    \n",
    "    try:\n",
    "        import kaggle\n",
    "    except ImportError:\n",
    "        print(\"Installing kaggle package...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"kaggle\"])\n",
    "        import kaggle\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\nDownloading {DATASET_NAME}...\")\n",
    "        subprocess.run([\n",
    "            \"kaggle\", \"datasets\", \"download\", \"-d\", DATASET_NAME, \"-p\", DATA_DIR\n",
    "        ], check=True)\n",
    "        \n",
    "        # Extract zip files\n",
    "        for zip_file in Path(DATA_DIR).glob(\"*.zip\"):\n",
    "            print(f\"Extracting {zip_file.name}...\")\n",
    "            with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "                zip_ref.extractall(DATA_DIR)\n",
    "            zip_file.unlink()\n",
    "        \n",
    "        print(\"✓ Dataset downloaded and extracted successfully!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")\n",
    "        print(f\"\\nPlease download manually from:\")\n",
    "        print(f\"https://www.kaggle.com/datasets/{DATASET_NAME}\")\n",
    "        return False\n",
    "\n",
    "# Check if dataset exists, download if needed\n",
    "csv_exists = any(Path(DATA_DIR).rglob(\"*.csv\"))\n",
    "if not csv_exists:\n",
    "    print(\"Dataset not found. Downloading...\")\n",
    "    download_dataset()\n",
    "else:\n",
    "    print(\"✓ Dataset already exists!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945c2bfc",
   "metadata": {},
   "source": [
    "## Section 4: Convert CSV to Binary Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3de7328f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "✓ PREPROCESSED DATA FOUND - SKIPPING CSV PARSING\n",
      "================================================================================\n",
      "Using cached file: cache/preprocessed/dataset.h5\n",
      "Size: 277.1 MB\n"
     ]
    }
   ],
   "source": [
    "def convert_csv_to_binary():\n",
    "    \"\"\"\n",
    "    Convert CSV files to a consolidated HDF5 dataset while preserving all features.\n",
    "    Adds source metadata and derived temporal features for downstream processing.\n",
    "    \"\"\"\n",
    "    preprocessed_file = HDF5_DATASET\n",
    "    \n",
    "    if preprocessed_file.exists() and USE_CACHED_DATA:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"✓ PREPROCESSED DATA FOUND - SKIPPING CSV PARSING\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Using cached file: {preprocessed_file}\")\n",
    "        print(f\"Size: {preprocessed_file.stat().st_size / 1024**2:.1f} MB\")\n",
    "        return preprocessed_file\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CONVERTING CSV TO EFFICIENT BINARY FORMAT\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Find all CSV files\n",
    "    csv_files = list(Path(DATA_DIR).rglob(\"*.csv\"))\n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(\"No CSV files found!\")\n",
    "    \n",
    "    print(f\"\\n✓ Found {len(csv_files)} CSV file(s):\")\n",
    "    for f in csv_files:\n",
    "        print(f\"  - {f.name} ({f.stat().st_size / 1024 / 1024:.1f} MB)\")\n",
    "    \n",
    "    # Load and combine all CSVs\n",
    "    if SAMPLE_SIZE:\n",
    "        print(f\"\\nLoading sample data (max {SAMPLE_SIZE:,} rows per file)...\")\n",
    "    else:\n",
    "        print(f\"\\nLoading FULL dataset (this may take a while)...\")\n",
    "    \n",
    "    dfs = []\n",
    "    manifest = []\n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file, nrows=SAMPLE_SIZE, low_memory=False)\n",
    "            original_rows = len(df)\n",
    "\n",
    "            # Normalize string columns to avoid mixed dtype issues\n",
    "            object_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "            for col in object_cols:\n",
    "                df[col] = df[col].astype(str).str.strip().fillna('__NA__')\n",
    "\n",
    "            # Attach source metadata\n",
    "            df['source_file'] = csv_file.name\n",
    "            df['source_path'] = str(csv_file.relative_to(DATA_DIR))\n",
    "            df['source_category'] = csv_file.parent.name\n",
    "\n",
    "            # Build temporal features without dropping original column\n",
    "            if 'frame.time' in df.columns:\n",
    "                time_str = df['frame.time'].astype(str).str.strip()\n",
    "                parsed_time = pd.to_datetime(time_str, format='%Y %H:%M:%S.%f', errors='coerce')\n",
    "                if parsed_time.isna().all():\n",
    "                    parsed_time = pd.to_datetime(time_str, errors='coerce')\n",
    "                parsed_time = parsed_time.fillna(method='ffill').fillna(method='bfill')\n",
    "                df['frame_time_datetime'] = parsed_time\n",
    "                base_time = parsed_time.iloc[0]\n",
    "                rel_seconds = (parsed_time - base_time).dt.total_seconds()\n",
    "                df['frame_time_relative_sec'] = rel_seconds.astype('float64')\n",
    "\n",
    "            dfs.append(df)\n",
    "            duration = float(df.get('frame_time_relative_sec', pd.Series([0])).max()) if len(df) else 0.0\n",
    "            manifest.append({\n",
    "                'file': str(csv_file.relative_to(DATA_DIR)),\n",
    "                'rows_loaded': int(original_rows),\n",
    "                'duration_seconds': duration\n",
    "            })\n",
    "            print(f\"  ✓ {csv_file.name}: {len(df):,} rows, {len(df.columns)} columns\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error loading {csv_file.name}: {e}\")\n",
    "    \n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Combined dataset: {len(df):,} rows × {len(df.columns)} columns\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(\"\\nPreparing data for HDF5 storage...\")\n",
    "    df_filtered = df.copy()\n",
    "    print(f\"Final dataset shape: {df_filtered.shape}\")\n",
    "    print(f\"Memory usage: {df_filtered.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "    preprocessed_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_filtered.to_hdf(preprocessed_file, key='data', mode='w', index=False)\n",
    "    print(f\"✓ Saved: {preprocessed_file}\")\n",
    "    print(f\"  Size: {preprocessed_file.stat().st_size / 1024**2:.1f} MB\")\n",
    "\n",
    "    total_csv_size = sum(f.stat().st_size for f in csv_files)\n",
    "    compression_ratio = (1 - preprocessed_file.stat().st_size / total_csv_size) * 100\n",
    "    print(f\"  Compression: {compression_ratio:.1f}% savings over CSV\")\n",
    "    print(f\"  Original CSV size: {total_csv_size / 1024**2:.1f} MB\")\n",
    "\n",
    "    manifest_path = Path(PREPROCESSED_DIR) / 'ingest_manifest.json'\n",
    "    with open(manifest_path, 'w') as f:\n",
    "        json.dump(manifest, f, indent=2)\n",
    "    print(f\"  Manifest saved: {manifest_path}\")\n",
    "\n",
    "    return preprocessed_file\n",
    "\n",
    "# Convert CSV to HDF5\n",
    "preprocessed_file = convert_csv_to_binary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559b7747",
   "metadata": {},
   "source": [
    "## Section 5: Explore Dataset Characteristics\n",
    "\n",
    "In this section, we'll load the preprocessed HDF5 data and explore the dataset characteristics including:\n",
    "- Basic statistics and data types\n",
    "- Class distribution analysis\n",
    "- Feature analysis and correlations\n",
    "- Temporal patterns and sequence characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d101ebbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXPLORING DATASET CHARACTERISTICS\n",
      "================================================================================\n",
      "\n",
      "Loading data from: cache/preprocessed/dataset.h5\n",
      "✓ Loaded 469,070 rows × 68 columns\n",
      "\n",
      "================================================================================\n",
      "BASIC DATASET INFO\n",
      "================================================================================\n",
      "Shape: (469070, 68)\n",
      "✓ Loaded 469,070 rows × 68 columns\n",
      "\n",
      "================================================================================\n",
      "BASIC DATASET INFO\n",
      "================================================================================\n",
      "Shape: (469070, 68)\n",
      "Memory usage: 864.43 MB\n",
      "\n",
      "Data types:\n",
      "  object: 34 columns\n",
      "  float64: 32 columns\n",
      "  int64: 1 columns\n",
      "  datetime64[ns]: 1 columns\n",
      "\n",
      "================================================================================\n",
      "TARGET VARIABLE ANALYSIS\n",
      "================================================================================\n",
      "Classes found: 16\n",
      "Most frequent class: 'Normal' (220,000 samples)\n",
      "Least frequent class: 'Fingerprinting' (1,001 samples)\n",
      "\n",
      "Class imbalance ratio: 219.78x\n",
      "⚠️  SEVERE class imbalance detected - consider balancing techniques\n",
      "\n",
      "================================================================================\n",
      "FEATURE ANALYSIS\n",
      "================================================================================\n",
      "Numeric features: 33\n",
      "Categorical features: 34\n",
      "Analysis features: 64\n",
      "\n",
      "Numeric feature statistics (first 10):\n",
      "Memory usage: 864.43 MB\n",
      "\n",
      "Data types:\n",
      "  object: 34 columns\n",
      "  float64: 32 columns\n",
      "  int64: 1 columns\n",
      "  datetime64[ns]: 1 columns\n",
      "\n",
      "================================================================================\n",
      "TARGET VARIABLE ANALYSIS\n",
      "================================================================================\n",
      "Classes found: 16\n",
      "Most frequent class: 'Normal' (220,000 samples)\n",
      "Least frequent class: 'Fingerprinting' (1,001 samples)\n",
      "\n",
      "Class imbalance ratio: 219.78x\n",
      "⚠️  SEVERE class imbalance detected - consider balancing techniques\n",
      "\n",
      "================================================================================\n",
      "FEATURE ANALYSIS\n",
      "================================================================================\n",
      "Numeric features: 33\n",
      "Categorical features: 34\n",
      "Analysis features: 64\n",
      "\n",
      "Numeric feature statistics (first 10):\n",
      "                            count         mean          std  min  25%  50%    75%           max\n",
      "arp.opcode              469070.00         0.01         0.18 0.00 0.00 0.00   0.00          6.00\n",
      "icmp.seq_le             469070.00       367.38      1914.23 0.00 0.00 0.00   0.00      15043.00\n",
      "icmp.transmit_timestamp 469070.00     27227.82   1447115.09 0.00 0.00 0.00   0.00   77289023.00\n",
      "icmp.unused             469070.00         0.00         0.00 0.00 0.00 0.00   0.00          0.00\n",
      "http.content_length     469070.00         9.30       143.03 0.00 0.00 0.00   0.00      83655.00\n",
      "http.response           469070.00         0.03         0.17 0.00 0.00 0.00   0.00          1.00\n",
      "http.tls_port           469070.00      1984.54     13760.77 0.00 0.00 0.00   0.00     161585.00\n",
      "tcp.ack                 469070.00 168847354.72 686595815.94 0.00 1.00 5.00 121.00 4283119198.00\n",
      "tcp.connection.rst      469070.00         0.11         0.32 0.00 0.00 0.00   0.00          1.00\n",
      "tcp.connection.syn      469070.00         0.12         5.16 0.00 0.00 0.00   0.00       1440.00\n",
      "\n",
      "================================================================================\n",
      "FEATURE CORRELATIONS\n",
      "================================================================================\n",
      "Using sample of 10,000 rows for correlation analysis\n",
      "\n",
      "Highly correlated feature pairs (|ρ| > 0.8):\n",
      "  mqtt.conflag.cleansess ↔ mqtt.proto_len: 1.000\n",
      "  tcp.flags.ack ↔ mqtt.ver: 0.958\n",
      "  mqtt.len ↔ mqtt.topic_len: 0.887\n",
      "  http.tls_port ↔ mbtcp.len: 0.843\n",
      "  tcp.ack ↔ mbtcp.trans_id: 0.809\n",
      "  mbtcp.len ↔ mbtcp.trans_id: 0.804\n",
      "\n",
      "================================================================================\n",
      "TEMPORAL ANALYSIS\n",
      "================================================================================\n",
      "Time range: 2446401600.00 seconds\n",
      "                            count         mean          std  min  25%  50%    75%           max\n",
      "arp.opcode              469070.00         0.01         0.18 0.00 0.00 0.00   0.00          6.00\n",
      "icmp.seq_le             469070.00       367.38      1914.23 0.00 0.00 0.00   0.00      15043.00\n",
      "icmp.transmit_timestamp 469070.00     27227.82   1447115.09 0.00 0.00 0.00   0.00   77289023.00\n",
      "icmp.unused             469070.00         0.00         0.00 0.00 0.00 0.00   0.00          0.00\n",
      "http.content_length     469070.00         9.30       143.03 0.00 0.00 0.00   0.00      83655.00\n",
      "http.response           469070.00         0.03         0.17 0.00 0.00 0.00   0.00          1.00\n",
      "http.tls_port           469070.00      1984.54     13760.77 0.00 0.00 0.00   0.00     161585.00\n",
      "tcp.ack                 469070.00 168847354.72 686595815.94 0.00 1.00 5.00 121.00 4283119198.00\n",
      "tcp.connection.rst      469070.00         0.11         0.32 0.00 0.00 0.00   0.00          1.00\n",
      "tcp.connection.syn      469070.00         0.12         5.16 0.00 0.00 0.00   0.00       1440.00\n",
      "\n",
      "================================================================================\n",
      "FEATURE CORRELATIONS\n",
      "================================================================================\n",
      "Using sample of 10,000 rows for correlation analysis\n",
      "\n",
      "Highly correlated feature pairs (|ρ| > 0.8):\n",
      "  mqtt.conflag.cleansess ↔ mqtt.proto_len: 1.000\n",
      "  tcp.flags.ack ↔ mqtt.ver: 0.958\n",
      "  mqtt.len ↔ mqtt.topic_len: 0.887\n",
      "  http.tls_port ↔ mbtcp.len: 0.843\n",
      "  tcp.ack ↔ mbtcp.trans_id: 0.809\n",
      "  mbtcp.len ↔ mbtcp.trans_id: 0.804\n",
      "\n",
      "================================================================================\n",
      "TEMPORAL ANALYSIS\n",
      "================================================================================\n",
      "Time range: 2446401600.00 seconds\n",
      "\n",
      "================================================================================\n",
      "EXPLORATION COMPLETE\n",
      "================================================================================\n",
      "✓ Dataset exploration saved to: visualizations\n",
      "\n",
      "================================================================================\n",
      "EXPLORATION COMPLETE\n",
      "================================================================================\n",
      "✓ Dataset exploration saved to: visualizations\n"
     ]
    }
   ],
   "source": [
    "def explore_dataset():\n",
    "    \"\"\"\n",
    "    Comprehensive dataset exploration including statistics, distributions, and correlations.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EXPLORING DATASET CHARACTERISTICS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load the preprocessed data\n",
    "    print(f\"\\nLoading data from: {preprocessed_file}\")\n",
    "    df = pd.read_hdf(preprocessed_file, key='data')\n",
    "    print(f\"✓ Loaded {len(df):,} rows × {len(df.columns)} columns\")\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"BASIC DATASET INFO\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Data types\n",
    "    print(f\"\\nData types:\")\n",
    "    dtype_counts = df.dtypes.value_counts()\n",
    "    for dtype, count in dtype_counts.items():\n",
    "        print(f\"  {dtype}: {count} columns\")\n",
    "    \n",
    "    # Target variable analysis\n",
    "    target_col = 'Attack_type'\n",
    "    if target_col in df.columns:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"TARGET VARIABLE ANALYSIS\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        class_counts = df[target_col].value_counts()\n",
    "        print(f\"Classes found: {len(class_counts)}\")\n",
    "        print(f\"Most frequent class: '{class_counts.index[0]}' ({class_counts.iloc[0]:,} samples)\")\n",
    "        print(f\"Least frequent class: '{class_counts.index[-1]}' ({class_counts.iloc[-1]:,} samples)\")\n",
    "        \n",
    "        # Class distribution plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        ax = class_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "        plt.title('Class Distribution', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Attack Type', fontsize=12)\n",
    "        plt.ylabel('Sample Count', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, v in enumerate(class_counts):\n",
    "            ax.text(i, v + max(class_counts) * 0.01, f'{v:,}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(VISUALIZATIONS_DIR / 'class_distribution.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Class imbalance analysis\n",
    "        imbalance_ratio = class_counts.max() / class_counts.min()\n",
    "        print(f\"\\nClass imbalance ratio: {imbalance_ratio:.2f}x\")\n",
    "        if imbalance_ratio > 10:\n",
    "            print(\"⚠️  SEVERE class imbalance detected - consider balancing techniques\")\n",
    "        elif imbalance_ratio > 5:\n",
    "            print(\"⚠️  Moderate class imbalance detected\")\n",
    "        else:\n",
    "            print(\"✓ Balanced class distribution\")\n",
    "    \n",
    "    # Feature analysis\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"FEATURE ANALYSIS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Identify feature types\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    print(f\"Numeric features: {len(numeric_cols)}\")\n",
    "    print(f\"Categorical features: {len(categorical_cols)}\")\n",
    "    \n",
    "    # Remove metadata columns for analysis\n",
    "    metadata_cols = ['source_file', 'source_path', 'source_category']\n",
    "    analysis_cols = [col for col in df.columns if col not in metadata_cols + [target_col]]\n",
    "    \n",
    "    print(f\"Analysis features: {len(analysis_cols)}\")\n",
    "    \n",
    "    # Numeric feature statistics\n",
    "    if numeric_cols:\n",
    "        print(f\"\\nNumeric feature statistics (first 10):\")\n",
    "        numeric_stats = df[numeric_cols[:10]].describe().T\n",
    "        print(numeric_stats.to_string(float_format='%.2f'))\n",
    "    \n",
    "    # Correlation analysis for numeric features\n",
    "    if len(numeric_cols) > 1:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"FEATURE CORRELATIONS\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Sample for correlation if dataset is large\n",
    "        sample_size = min(10000, len(df))\n",
    "        if sample_size < len(df):\n",
    "            corr_df = df[numeric_cols].sample(sample_size, random_state=42)\n",
    "            print(f\"Using sample of {sample_size:,} rows for correlation analysis\")\n",
    "        else:\n",
    "            corr_df = df[numeric_cols]\n",
    "        \n",
    "        corr_matrix = corr_df.corr()\n",
    "        \n",
    "        # Plot correlation heatmap\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "        sns.heatmap(corr_matrix, mask=mask, annot=False, cmap='coolwarm', \n",
    "                   center=0, square=True, linewidths=0.5)\n",
    "        plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(VISUALIZATIONS_DIR / 'feature_correlations.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Find highly correlated features\n",
    "        high_corr = []\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i+1, len(corr_matrix.columns)):\n",
    "                corr_val = abs(corr_matrix.iloc[i, j])\n",
    "                if corr_val > 0.8:\n",
    "                    high_corr.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_val))\n",
    "        \n",
    "        if high_corr:\n",
    "            print(f\"\\nHighly correlated feature pairs (|ρ| > 0.8):\")\n",
    "            for feat1, feat2, corr in sorted(high_corr, key=lambda x: x[2], reverse=True)[:10]:\n",
    "                print(f\"  {feat1} ↔ {feat2}: {corr:.3f}\")\n",
    "        else:\n",
    "            print(\"\\nNo highly correlated feature pairs found (|ρ| > 0.8)\")\n",
    "    \n",
    "    # Temporal analysis\n",
    "    if 'frame_time_relative_sec' in df.columns:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"TEMPORAL ANALYSIS\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        time_col = 'frame_time_relative_sec'\n",
    "        time_range = df[time_col].max() - df[time_col].min()\n",
    "        print(f\"Time range: {time_range:.2f} seconds\")\n",
    "        \n",
    "        # Sample time series plot\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        \n",
    "        # Plot a few numeric features over time (sample)\n",
    "        sample_df = df.sample(min(5000, len(df)), random_state=42).sort_values(time_col)\n",
    "        plot_cols = [col for col in numeric_cols[:3] if col in sample_df.columns]\n",
    "        \n",
    "        if plot_cols:\n",
    "            for i, col in enumerate(plot_cols):\n",
    "                plt.subplot(1, len(plot_cols), i+1)\n",
    "                plt.plot(sample_df[time_col], sample_df[col], alpha=0.7, linewidth=1)\n",
    "                plt.title(f'{col} over Time', fontsize=12)\n",
    "                plt.xlabel('Time (seconds)')\n",
    "                plt.ylabel(col)\n",
    "                plt.grid(alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(VISUALIZATIONS_DIR / 'temporal_patterns.png', dpi=150, bbox_inches='tight')\n",
    "            plt.show()\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EXPLORATION COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"✓ Dataset exploration saved to: {VISUALIZATIONS_DIR}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Explore the dataset\n",
    "df = explore_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abda373f",
   "metadata": {},
   "source": [
    "## Section 6: Prepare Time Series Data\n",
    "\n",
    "In this section, we'll prepare the data for time series modeling by:\n",
    "- Encoding categorical features\n",
    "- Creating sliding window sequences\n",
    "- Splitting data into train/validation/test sets\n",
    "- Applying feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "92c30896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PREPARING TIME SERIES SEQUENCES\n",
      "================================================================================\n",
      "Total features: 63\n",
      "  Numeric: 33\n",
      "  Categorical: 30\n",
      "\n",
      "Encoding 30 categorical features...\n",
      "✓ Encoder saved: cache/ordinal_encoder.pkl\n",
      "✓ Label encoder saved: cache/label_encoder.pkl\n",
      "\n",
      "Feature matrix shape: (469070, 63)\n",
      "Target vector shape: (469070,)\n",
      "Number of classes: 16\n",
      "\n",
      "================================================================================\n",
      "CREATING TIME SERIES SEQUENCES\n",
      "================================================================================\n",
      "Sequence length: 128 timesteps\n",
      "Sorting data by timestamp...\n",
      "✓ Encoder saved: cache/ordinal_encoder.pkl\n",
      "✓ Label encoder saved: cache/label_encoder.pkl\n",
      "\n",
      "Feature matrix shape: (469070, 63)\n",
      "Target vector shape: (469070,)\n",
      "Number of classes: 16\n",
      "\n",
      "================================================================================\n",
      "CREATING TIME SERIES SEQUENCES\n",
      "================================================================================\n",
      "Sequence length: 128 timesteps\n",
      "Sorting data by timestamp...\n",
      "✓ Created 468,943 sequences\n",
      "  Sequence shape: (468943, 128, 63)\n",
      "  Target shape: (468943,)\n",
      "\n",
      "Downsampling sequences to 20,000 for quicker experimentation...\n",
      "  -> Using 20,000 sequences after downsampling\n",
      "\n",
      "================================================================================\n",
      "SPLITTING DATA INTO TRAIN/VAL/TEST SETS\n",
      "================================================================================\n",
      "✓ Created 468,943 sequences\n",
      "  Sequence shape: (468943, 128, 63)\n",
      "  Target shape: (468943,)\n",
      "\n",
      "Downsampling sequences to 20,000 for quicker experimentation...\n",
      "  -> Using 20,000 sequences after downsampling\n",
      "\n",
      "================================================================================\n",
      "SPLITTING DATA INTO TRAIN/VAL/TEST SETS\n",
      "================================================================================\n",
      "Train set: 12,800 sequences\n",
      "Validation set: 3,200 sequences\n",
      "Test set: 4,000 sequences\n",
      "\n",
      "Applying RobustScaler to features...\n",
      "Train set: 12,800 sequences\n",
      "Validation set: 3,200 sequences\n",
      "Test set: 4,000 sequences\n",
      "\n",
      "Applying RobustScaler to features...\n",
      "✓ Scaler saved: cache/robust_scaler.pkl\n",
      "\n",
      "Train class distribution: {14: 545, 1: 546, 2: 546, 3: 546, 10: 546, 9: 546, 15: 434, 7: 6005, 12: 546, 13: 733, 4: 542, 0: 546, 11: 597, 8: 28, 6: 66, 5: 28}\n",
      "Val class distribution: {7: 1502, 15: 109, 14: 137, 10: 136, 12: 136, 6: 17, 3: 137, 11: 149, 9: 136, 4: 136, 0: 136, 13: 183, 1: 136, 2: 136, 8: 7, 5: 7}\n",
      "Test class distribution: {7: 1876, 14: 171, 10: 171, 3: 170, 5: 8, 9: 171, 12: 171, 0: 171, 2: 171, 11: 186, 4: 169, 1: 171, 15: 136, 13: 229, 6: 21, 8: 8}\n",
      "\n",
      "================================================================================\n",
      "SAVING PROCESSED DATA\n",
      "================================================================================\n",
      "✓ Processed data saved to cache directory\n",
      "✓ Metadata saved: cache/preprocessing_metadata.json\n",
      "✓ Scaler saved: cache/robust_scaler.pkl\n",
      "\n",
      "Train class distribution: {14: 545, 1: 546, 2: 546, 3: 546, 10: 546, 9: 546, 15: 434, 7: 6005, 12: 546, 13: 733, 4: 542, 0: 546, 11: 597, 8: 28, 6: 66, 5: 28}\n",
      "Val class distribution: {7: 1502, 15: 109, 14: 137, 10: 136, 12: 136, 6: 17, 3: 137, 11: 149, 9: 136, 4: 136, 0: 136, 13: 183, 1: 136, 2: 136, 8: 7, 5: 7}\n",
      "Test class distribution: {7: 1876, 14: 171, 10: 171, 3: 170, 5: 8, 9: 171, 12: 171, 0: 171, 2: 171, 11: 186, 4: 169, 1: 171, 15: 136, 13: 229, 6: 21, 8: 8}\n",
      "\n",
      "================================================================================\n",
      "SAVING PROCESSED DATA\n",
      "================================================================================\n",
      "✓ Processed data saved to cache directory\n",
      "✓ Metadata saved: cache/preprocessing_metadata.json\n"
     ]
    }
   ],
   "source": [
    "def prepare_time_series_data(df):\n",
    "    \"\"\"\n",
    "    Prepare time series sequences with categorical encoding and sliding windows.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PREPARING TIME SERIES SEQUENCES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Identify feature columns (exclude metadata and target)\n",
    "    metadata_cols = ['source_file', 'source_path', 'source_category', 'frame_time_datetime']\n",
    "    target_col = 'Attack_type'\n",
    "    \n",
    "    # Get all features except metadata and target\n",
    "    all_cols = [col for col in df.columns if col not in metadata_cols + [target_col]]\n",
    "    \n",
    "    # Separate numeric and categorical features\n",
    "    numeric_cols = df[all_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = [col for col in all_cols if col not in numeric_cols]\n",
    "    \n",
    "    print(f\"Total features: {len(all_cols)}\")\n",
    "    print(f\"  Numeric: {len(numeric_cols)}\")\n",
    "    print(f\"  Categorical: {len(categorical_cols)}\")\n",
    "    \n",
    "    # Handle categorical encoding\n",
    "    if categorical_cols:\n",
    "        print(f\"\\nEncoding {len(categorical_cols)} categorical features...\")\n",
    "        encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "        df_encoded = df.copy()\n",
    "        df_encoded[categorical_cols] = encoder.fit_transform(df[categorical_cols].astype(str))\n",
    "        \n",
    "        # Save encoder for later use\n",
    "        encoder_path = CACHE_DIR / 'ordinal_encoder.pkl'\n",
    "        with open(encoder_path, 'wb') as f:\n",
    "            pickle.dump(encoder, f)\n",
    "        print(f\"✓ Encoder saved: {encoder_path}\")\n",
    "    else:\n",
    "        df_encoded = df.copy()\n",
    "    \n",
    "    # Prepare features and target\n",
    "    feature_cols = numeric_cols + categorical_cols\n",
    "    X = df_encoded[feature_cols].values.astype(np.float32)\n",
    "    y = df_encoded[target_col].values\n",
    "    \n",
    "    # Encode target labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    # Save label encoder\n",
    "    label_encoder_path = CACHE_DIR / 'label_encoder.pkl'\n",
    "    with open(label_encoder_path, 'wb') as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "    print(f\"✓ Label encoder saved: {label_encoder_path}\")\n",
    "    \n",
    "    print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "    print(f\"Target vector shape: {y_encoded.shape}\")\n",
    "    print(f\"Number of classes: {len(label_encoder.classes_)}\")\n",
    "    \n",
    "    # Create time series sequences\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"CREATING TIME SERIES SEQUENCES\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    sequence_length = SEQUENCE_LENGTH\n",
    "    print(f\"Sequence length: {sequence_length} timesteps\")\n",
    "    \n",
    "    # Sort by time if available to preserve temporal structure\n",
    "    if 'frame_time_relative_sec' in df_encoded.columns:\n",
    "        print(\"Sorting data by timestamp...\")\n",
    "        sort_indices = df_encoded['frame_time_relative_sec'].argsort()\n",
    "        X = X[sort_indices]\n",
    "        y_encoded = y_encoded[sort_indices]\n",
    "    \n",
    "    # Create sliding window sequences\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    for i in range(len(X) - sequence_length + 1):\n",
    "        seq = X[i:i + sequence_length]\n",
    "        target = y_encoded[i + sequence_length - 1]  # Target is the last timestep's label\n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "    \n",
    "    X_sequences = np.asarray(sequences, dtype=np.float32)\n",
    "    y_sequences = np.asarray(targets, dtype=np.int64)\n",
    "    \n",
    "    print(f\"✓ Created {len(X_sequences):,} sequences\")\n",
    "    print(f\"  Sequence shape: {X_sequences.shape}\")\n",
    "    print(f\"  Target shape: {y_sequences.shape}\")\n",
    "    \n",
    "    # Optional downsampling for faster experimentation\n",
    "    if MAX_SEQUENCES and len(X_sequences) > MAX_SEQUENCES:\n",
    "        print(f\"\\nDownsampling sequences to {MAX_SEQUENCES:,} for quicker experimentation...\")\n",
    "        sample_indices, _ = train_test_split(\n",
    "            np.arange(len(X_sequences)),\n",
    "            train_size=MAX_SEQUENCES,\n",
    "            random_state=RANDOM_STATE,\n",
    "            stratify=y_sequences,\n",
    "            shuffle=True\n",
    "        )\n",
    "        X_sequences = X_sequences[sample_indices]\n",
    "        y_sequences = y_sequences[sample_indices]\n",
    "        print(f\"  -> Using {len(X_sequences):,} sequences after downsampling\")\n",
    "    \n",
    "    # Train/validation/test split with stratification to preserve class balance\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"SPLITTING DATA INTO TRAIN/VAL/TEST SETS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X_sequences,\n",
    "        y_sequences,\n",
    "        test_size=0.2,\n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=y_sequences,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val,\n",
    "        y_train_val,\n",
    "        test_size=0.2,\n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=y_train_val,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Train set: {X_train.shape[0]:,} sequences\")\n",
    "    print(f\"Validation set: {X_val.shape[0]:,} sequences\")\n",
    "    print(f\"Test set: {X_test.shape[0]:,} sequences\")\n",
    "    \n",
    "    # Apply feature scaling using training data only\n",
    "    print(f\"\\nApplying RobustScaler to features...\")\n",
    "    scaler = RobustScaler()\n",
    "    scaler.fit(X_train.reshape(-1, X_train.shape[-1]))\n",
    "    \n",
    "    def scale_sequences(data):\n",
    "        flattened = data.reshape(-1, data.shape[-1])\n",
    "        scaled = scaler.transform(flattened)\n",
    "        scaled = np.nan_to_num(scaled, nan=0.0, posinf=0.0, neginf=0.0, copy=False)\n",
    "        return scaled.reshape(data.shape).astype(np.float32)\n",
    "    \n",
    "    X_train_scaled = scale_sequences(X_train)\n",
    "    X_val_scaled = scale_sequences(X_val)\n",
    "    X_test_scaled = scale_sequences(X_test)\n",
    "    \n",
    "    # Save scaler\n",
    "    scaler_path = CACHE_DIR / 'robust_scaler.pkl'\n",
    "    with open(scaler_path, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    print(f\"✓ Scaler saved: {scaler_path}\")\n",
    "    \n",
    "    # Class distribution in splits\n",
    "    def summarize_distribution(name, labels):\n",
    "        counts = Counter(labels)\n",
    "        human_counts = {int(k): int(v) for k, v in counts.items()}\n",
    "        print(f\"{name} class distribution: {human_counts}\")\n",
    "    \n",
    "    print()\n",
    "    summarize_distribution(\"Train\", y_train)\n",
    "    summarize_distribution(\"Val\", y_val)\n",
    "    summarize_distribution(\"Test\", y_test)\n",
    "    \n",
    "    # Save processed data\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"SAVING PROCESSED DATA\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    np.save(CACHE_DIR / 'X_train.npy', X_train_scaled)\n",
    "    np.save(CACHE_DIR / 'X_val.npy', X_val_scaled)\n",
    "    np.save(CACHE_DIR / 'X_test.npy', X_test_scaled)\n",
    "    np.save(CACHE_DIR / 'y_train.npy', y_train)\n",
    "    np.save(CACHE_DIR / 'y_val.npy', y_val)\n",
    "    np.save(CACHE_DIR / 'y_test.npy', y_test)\n",
    "    \n",
    "    print(\"✓ Processed data saved to cache directory\")\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'sequence_length': sequence_length,\n",
    "        'num_features': len(feature_cols),\n",
    "        'num_classes': len(label_encoder.classes_),\n",
    "        'feature_cols': feature_cols,\n",
    "        'numeric_cols': numeric_cols,\n",
    "        'categorical_cols': categorical_cols,\n",
    "        'class_names': label_encoder.classes_.tolist(),\n",
    "        'train_samples': int(X_train.shape[0]),\n",
    "        'val_samples': int(X_val.shape[0]),\n",
    "        'test_samples': int(X_test.shape[0]),\n",
    "        'total_sequences_generated': int(len(sequences)),\n",
    "        'total_sequences_used': int(X_sequences.shape[0])\n",
    "    }\n",
    "    \n",
    "    metadata_path = CACHE_DIR / 'preprocessing_metadata.json'\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"✓ Metadata saved: {metadata_path}\")\n",
    "    \n",
    "    return X_train_scaled, X_val_scaled, X_test_scaled, y_train, y_val, y_test, metadata\n",
    "\n",
    "# Prepare time series data\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, metadata = prepare_time_series_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04215da",
   "metadata": {},
   "source": [
    "## Section 7: Build DeepFed Model\n",
    "\n",
    "In this section, we'll construct the DeepFed architecture which combines:\n",
    "- **GRU layers** for temporal pattern recognition in sequential data\n",
    "- **CNN layers** for spatial feature extraction\n",
    "- **MLP layers** for final classification\n",
    "\n",
    "The model processes time series sequences and learns both temporal dependencies and spatial patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dea7f337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BUILDING DEEPFED MODEL\n",
      "================================================================================\n",
      "Input shape: (128, 63)\n",
      "Number of classes: 16\n",
      "\n",
      "Building GRU branch...\n",
      "Building CNN branch...\n",
      "Fusing branches...\n",
      "Building MLP classifier...\n",
      "\n",
      "================================================================================\n",
      "MODEL SUMMARY\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"DeepFed\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"DeepFed\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequences     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,320</span> │ input_sequences[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">74,112</span> │ input_sequences[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ maxpool_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru_dropout_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ gru_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,640</span> │ maxpool_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">37,248</span> │ gru_dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ maxpool_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru_dropout_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ gru_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_avg_pool     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ maxpool_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ gru_dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ global_avg_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_dropout_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ mlp_dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_dropout_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,064</span> │ mlp_dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequences     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m63\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m24,320\u001b[0m │ input_sequences[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m74,112\u001b[0m │ input_sequences[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ maxpool_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ conv1d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru_dropout_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ gru_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │     \u001b[38;5;34m24,640\u001b[0m │ maxpool_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru_2 (\u001b[38;5;33mGRU\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m37,248\u001b[0m │ gru_dropout_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ maxpool_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ conv1d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru_dropout_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ gru_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_avg_pool     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ maxpool_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ gru_dropout_2[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ global_avg_pool[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m33,024\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_dropout_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m32,896\u001b[0m │ mlp_dropout_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_dropout_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │      \u001b[38;5;34m2,064\u001b[0m │ mlp_dropout_2[\u001b[38;5;34m0\u001b[0m]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">228,304</span> (891.81 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m228,304\u001b[0m (891.81 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">228,304</span> (891.81 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m228,304\u001b[0m (891.81 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total parameters: 228,304\n",
      "Estimated model size: 0.87 MB\n"
     ]
    }
   ],
   "source": [
    "def build_deepfed_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Build the DeepFed model with GRU, CNN, and MLP components.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"BUILDING DEEPFED MODEL\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"Input shape: {input_shape}\")\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "    \n",
    "    # Input layer\n",
    "    inputs = layers.Input(shape=input_shape, name='input_sequences')\n",
    "    \n",
    "    # GRU branch for temporal patterns\n",
    "    print(\"\\nBuilding GRU branch...\")\n",
    "    gru_branch = layers.GRU(units=GRU_UNITS, return_sequences=True, name='gru_1')(inputs)\n",
    "    gru_branch = layers.Dropout(GRU_DROPOUT, name='gru_dropout_1')(gru_branch)\n",
    "    gru_branch = layers.GRU(units=GRU_UNITS // 2, return_sequences=False, name='gru_2')(gru_branch)\n",
    "    gru_branch = layers.Dropout(GRU_DROPOUT, name='gru_dropout_2')(gru_branch)\n",
    "    \n",
    "    # CNN branch for spatial patterns\n",
    "    print(\"Building CNN branch...\")\n",
    "    cnn_branch = layers.Conv1D(filters=CNN_FILTERS, kernel_size=CNN_KERNEL_SIZE, \n",
    "                               activation='relu', padding='same', name='conv1d_1')(inputs)\n",
    "    cnn_branch = layers.MaxPooling1D(pool_size=2, name='maxpool_1')(cnn_branch)\n",
    "    cnn_branch = layers.Conv1D(filters=CNN_FILTERS // 2, kernel_size=CNN_KERNEL_SIZE, \n",
    "                               activation='relu', padding='same', name='conv1d_2')(cnn_branch)\n",
    "    cnn_branch = layers.MaxPooling1D(pool_size=2, name='maxpool_2')(cnn_branch)\n",
    "    cnn_branch = layers.GlobalAveragePooling1D(name='global_avg_pool')(cnn_branch)\n",
    "    \n",
    "    # Concatenate branches\n",
    "    print(\"Fusing branches...\")\n",
    "    concatenated = layers.Concatenate(name='concatenate')([gru_branch, cnn_branch])\n",
    "    \n",
    "    # MLP head\n",
    "    print(\"Building MLP classifier...\")\n",
    "    mlp = layers.Dense(MLP_UNITS, activation='relu', name='dense_1')(concatenated)\n",
    "    mlp = layers.Dropout(MLP_DROPOUT, name='mlp_dropout_1')(mlp)\n",
    "    mlp = layers.Dense(MLP_UNITS // 2, activation='relu', name='dense_2')(mlp)\n",
    "    mlp = layers.Dropout(MLP_DROPOUT, name='mlp_dropout_2')(mlp)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = layers.Dense(num_classes, activation='softmax', name='output')(mlp)\n",
    "    \n",
    "    # Create model\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='DeepFed')\n",
    "    \n",
    "    # Compile model\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"MODEL SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    model.summary()\n",
    "    \n",
    "    # Calculate model size\n",
    "    total_params = model.count_params()\n",
    "    print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "    \n",
    "    # Estimate model size in memory\n",
    "    param_size = 4  # float32 = 4 bytes\n",
    "    model_size_mb = (total_params * param_size) / (1024 ** 2)\n",
    "    print(f\"Estimated model size: {model_size_mb:.2f} MB\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "input_shape = (metadata['sequence_length'], metadata['num_features'])\n",
    "num_classes = metadata['num_classes']\n",
    "model = build_deepfed_model(input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5abd1f7",
   "metadata": {},
   "source": [
    "## Section 8: Train the Model\n",
    "\n",
    "In this section, we'll train the DeepFed model with:\n",
    "- Early stopping to prevent overfitting\n",
    "- Model checkpointing to save the best weights\n",
    "- Learning rate scheduling\n",
    "- Training history visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e0f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING DEEPFED MODEL\n",
      "================================================================================\n",
      "Training configuration:\n",
      "  Batch size: 128\n",
      "  Max epochs: 10\n",
      "  Early stopping patience: 5\n",
      "  Callbacks: ['EarlyStopping', 'TerminateOnNaN', 'ModelCheckpoint', 'ReduceLROnPlateau', 'CSVLogger']\n",
      "\n",
      "Class weights: {0: 1.465201465201465, 1: 1.465201465201465, 2: 1.465201465201465, 3: 1.465201465201465, 4: 1.4760147601476015, 5: 28.571428571428573, 6: 12.121212121212121, 7: 0.13322231473771856, 8: 28.571428571428573, 9: 1.465201465201465, 10: 1.465201465201465, 11: 1.340033500837521, 12: 1.465201465201465, 13: 1.0914051841746248, 14: 1.4678899082568808, 15: 1.8433179723502304}\n",
      "\n",
      "================================================================================\n",
      "STARTING TRAINING\n",
      "================================================================================\n",
      "Epoch 1/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.1106 - loss: 111361.8287\n",
      "Epoch 1: val_accuracy improved from None to 0.21219, saving model to models/deepfed/deepfed_best.weights.h5\n",
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.21219, saving model to models/deepfed/deepfed_best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 218ms/step - accuracy: 0.1164 - loss: 48310.4492 - val_accuracy: 0.2122 - val_loss: 1985.0671 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 218ms/step - accuracy: 0.1164 - loss: 48310.4492 - val_accuracy: 0.2122 - val_loss: 1985.0671 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m 80/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m4s\u001b[0m 202ms/step - accuracy: 0.1584 - loss: 6064.4155"
     ]
    }
   ],
   "source": [
    "def train_model(model, X_train, X_val, y_train, y_val):\n",
    "    \"\"\"\n",
    "    Train the DeepFed model with callbacks and monitoring.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TRAINING DEEPFED MODEL\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create callbacks\n",
    "    callbacks_list = []\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping = callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=EARLY_STOPPING_PATIENCE,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks_list.append(early_stopping)\n",
    "    \n",
    "    # Terminate on NaN to avoid wasted epochs\n",
    "    terminate_on_nan = callbacks.TerminateOnNaN()\n",
    "    callbacks_list.append(terminate_on_nan)\n",
    "    \n",
    "    # Model checkpointing (weights only for portability)\n",
    "    checkpoint_path = MODELS_DIR / 'deepfed_best.weights.h5'\n",
    "    checkpoint_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    model_checkpoint = callbacks.ModelCheckpoint(\n",
    "        filepath=str(checkpoint_path),\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks_list.append(model_checkpoint)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    lr_scheduler = callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-5,\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks_list.append(lr_scheduler)\n",
    "    \n",
    "    # CSV logger\n",
    "    csv_logger = callbacks.CSVLogger(str(MODELS_DIR / 'training_log.csv'), append=False)\n",
    "    callbacks_list.append(csv_logger)\n",
    "    \n",
    "    print(\"Training configuration:\")\n",
    "    print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"  Max epochs: {MAX_EPOCHS}\")\n",
    "    print(f\"  Early stopping patience: {EARLY_STOPPING_PATIENCE}\")\n",
    "    print(f\"  Callbacks: {[cb.__class__.__name__ for cb in callbacks_list]}\")\n",
    "    \n",
    "    # Calculate class weights for imbalanced data\n",
    "    unique_classes = np.unique(y_train)\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=unique_classes,\n",
    "        y=y_train\n",
    "    )\n",
    "    class_weights_dict = {int(cls): float(weight) for cls, weight in zip(unique_classes, class_weights)}\n",
    "    \n",
    "    print(f\"\\nClass weights: {class_weights_dict}\")\n",
    "    \n",
    "    # Quick NaN/infinity check before training\n",
    "    def assert_finite(name, array):\n",
    "        if not np.isfinite(array).all():\n",
    "            raise ValueError(f\"{name} contains NaN or infinite values after preprocessing\")\n",
    "    \n",
    "    assert_finite(\"X_train\", X_train)\n",
    "    assert_finite(\"X_val\", X_val)\n",
    "    \n",
    "    # Train the model\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"STARTING TRAINING\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=MAX_EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=callbacks_list,\n",
    "        class_weight=class_weights_dict,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\nTraining completed in {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
    "    \n",
    "    # Load best weights if available\n",
    "    if checkpoint_path.exists():\n",
    "        print(f\"\\nLoading best weights from: {checkpoint_path}\")\n",
    "        model.load_weights(str(checkpoint_path))\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Train the model\n",
    "model, history = train_model(model, X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48b3f87",
   "metadata": {},
   "source": [
    "## Section 9: Evaluate Model Performance\n",
    "\n",
    "In this section, we'll evaluate the trained model by:\n",
    "- Computing performance metrics on the test set\n",
    "- Generating classification reports and confusion matrices\n",
    "- Visualizing training history\n",
    "- Analyzing model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34b879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, history, metadata):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation with metrics, visualizations, and analysis.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EVALUATING MODEL PERFORMANCE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load label encoder for class names\n",
    "    label_encoder_path = CACHE_DIR / 'label_encoder.pkl'\n",
    "    with open(label_encoder_path, 'rb') as f:\n",
    "        label_encoder = pickle.load(f)\n",
    "    \n",
    "    class_names = label_encoder.classes_\n",
    "    \n",
    "    # Make predictions\n",
    "    print(\"Generating predictions on test set...\")\n",
    "    y_pred_prob = model.predict(X_test, batch_size=BATCH_SIZE, verbose=1)\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "    \n",
    "    # Basic metrics\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"CLASSIFICATION REPORT\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    report = classification_report(y_test, y_pred, target_names=class_names, output_dict=True)\n",
    "    print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"CONFUSION MATRIX\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "               xticklabels=class_names, yticklabels=class_names,\n",
    "               cbar_kws={'label': 'Number of Samples'})\n",
    "    plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(VISUALIZATIONS_DIR / 'confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Training history visualization\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TRAINING HISTORY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Accuracy\n",
    "    ax1.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "    ax1.set_title('Model Accuracy', fontweight='bold')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(alpha=0.3)\n",
    "    \n",
    "    # Loss\n",
    "    ax2.plot(history.history['loss'], label='Train Loss')\n",
    "    ax2.plot(history.history['val_loss'], label='Val Loss')\n",
    "    ax2.set_title('Model Loss', fontweight='bold')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(alpha=0.3)\n",
    "    \n",
    "    # Learning rate (if available)\n",
    "    if 'lr' in history.history:\n",
    "        ax3.plot(history.history['lr'])\n",
    "        ax3.set_title('Learning Rate Schedule', fontweight='bold')\n",
    "        ax3.set_xlabel('Epoch')\n",
    "        ax3.set_ylabel('Learning Rate')\n",
    "        ax3.set_yscale('log')\n",
    "        ax3.grid(alpha=0.3)\n",
    "    else:\n",
    "        ax3.text(0.5, 0.5, 'Learning rate data not available', \n",
    "                ha='center', va='center', transform=ax3.transAxes)\n",
    "        ax3.set_title('Learning Rate Schedule', fontweight='bold')\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    class_accuracies = []\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        class_mask = (y_test == i)\n",
    "        if np.sum(class_mask) > 0:\n",
    "            class_acc = np.mean(y_pred[class_mask] == i)\n",
    "            class_accuracies.append((class_name, class_acc))\n",
    "    \n",
    "    if class_accuracies:\n",
    "        classes, accs = zip(*class_accuracies)\n",
    "        bars = ax4.barh(classes, accs, color='skyblue', edgecolor='black')\n",
    "        ax4.set_title('Per-Class Accuracy', fontweight='bold')\n",
    "        ax4.set_xlabel('Accuracy')\n",
    "        ax4.set_xlim(0, 1)\n",
    "        ax4.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, acc in zip(bars, accs):\n",
    "            ax4.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                    f'{acc:.3f}', ha='left', va='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(VISUALIZATIONS_DIR / 'training_history.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Save model and results\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"SAVING MODEL AND RESULTS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Save final model\n",
    "    model_path = MODELS_DIR / 'deepfed_final_model.h5'\n",
    "    model.save(str(model_path))\n",
    "    print(f\"✓ Model saved: {model_path}\")\n",
    "    \n",
    "    # Save evaluation results\n",
    "    results = {\n",
    "        'test_loss': float(test_loss),\n",
    "        'test_accuracy': float(test_accuracy),\n",
    "        'classification_report': report,\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'class_names': class_names.tolist(),\n",
    "        'training_epochs': len(history.history['loss']),\n",
    "        'final_train_accuracy': float(history.history['accuracy'][-1]),\n",
    "        'final_val_accuracy': float(history.history['val_accuracy'][-1]),\n",
    "        'best_val_accuracy': float(max(history.history['val_accuracy']))\n",
    "    }\n",
    "    \n",
    "    results_path = MODELS_DIR / 'evaluation_results.json'\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"✓ Results saved: {results_path}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EVALUATION COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"✓ All results saved to: {VISUALIZATIONS_DIR}\")\n",
    "    print(f\"✓ Models saved to: {MODELS_DIR}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate the model\n",
    "results = evaluate_model(model, X_test, y_test, history, metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6354d502",
   "metadata": {},
   "source": [
    "## Section 10: Save Artifacts and Summary\n",
    "\n",
    "In this final section, we'll:\n",
    "- Save all trained models and preprocessing artifacts\n",
    "- Generate a comprehensive experiment summary\n",
    "- Provide next steps and recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e08729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_artifacts_and_summary(results, metadata):\n",
    "    \"\"\"\n",
    "    Save all artifacts and generate comprehensive experiment summary.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SAVING ARTIFACTS AND GENERATING SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create experiment summary\n",
    "    experiment_summary = {\n",
    "        'experiment_name': 'DeepFed_Edge_IIoT_Analysis',\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'dataset': {\n",
    "            'name': 'Edge-IIoTset',\n",
    "            'source': str(DATA_DIR),\n",
    "            'total_samples': metadata['total_sequences'],\n",
    "            'sequence_length': metadata['sequence_length'],\n",
    "            'num_features': metadata['num_features'],\n",
    "            'num_classes': metadata['num_classes'],\n",
    "            'class_names': metadata['class_names']\n",
    "        },\n",
    "        'model': {\n",
    "            'architecture': 'DeepFed (GRU+CNN+MLP)',\n",
    "            'input_shape': f\"({metadata['sequence_length']}, {metadata['num_features']})\",\n",
    "            'parameters': model.count_params(),\n",
    "            'gru_units': GRU_UNITS,\n",
    "            'cnn_filters': CNN_FILTERS,\n",
    "            'mlp_units': MLP_UNITS\n",
    "        },\n",
    "        'training': {\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'max_epochs': MAX_EPOCHS,\n",
    "            'learning_rate': LEARNING_RATE,\n",
    "            'early_stopping_patience': EARLY_STOPPING_PATIENCE,\n",
    "            'epochs_completed': len(history.history['loss']),\n",
    "            'final_train_accuracy': float(history.history['accuracy'][-1]),\n",
    "            'final_val_accuracy': float(history.history['val_accuracy'][-1]),\n",
    "            'best_val_accuracy': float(max(history.history['val_accuracy']))\n",
    "        },\n",
    "        'evaluation': {\n",
    "            'test_accuracy': results['test_accuracy'],\n",
    "            'test_loss': results['test_loss'],\n",
    "            'precision_macro': results['classification_report']['macro avg']['precision'],\n",
    "            'recall_macro': results['classification_report']['macro avg']['recall'],\n",
    "            'f1_macro': results['classification_report']['macro avg']['f1-score']\n",
    "        },\n",
    "        'artifacts': {\n",
    "            'model_path': str(MODELS_DIR / 'deepfed_final_model.h5'),\n",
    "            'weights_path': str(MODELS_DIR / 'deepfed_best_weights.h5'),\n",
    "            'scaler_path': str(CACHE_DIR / 'robust_scaler.pkl'),\n",
    "            'encoder_path': str(CACHE_DIR / 'ordinal_encoder.pkl'),\n",
    "            'label_encoder_path': str(CACHE_DIR / 'label_encoder.pkl'),\n",
    "            'training_log': str(MODELS_DIR / 'training_log.csv'),\n",
    "            'evaluation_results': str(MODELS_DIR / 'evaluation_results.json'),\n",
    "            'visualizations_dir': str(VISUALIZATIONS_DIR)\n",
    "        },\n",
    "        'configuration': {\n",
    "            'use_cached_data': USE_CACHED_DATA,\n",
    "            'sample_size': SAMPLE_SIZE,\n",
    "            'sequence_length': SEQUENCE_LENGTH,\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'learning_rate': LEARNING_RATE,\n",
    "            'gru_units': GRU_UNITS,\n",
    "            'gru_dropout': GRU_DROPOUT,\n",
    "            'cnn_filters': CNN_FILTERS,\n",
    "            'cnn_kernel_size': CNN_KERNEL_SIZE,\n",
    "            'mlp_units': MLP_UNITS,\n",
    "            'mlp_dropout': MLP_DROPOUT,\n",
    "            'max_epochs': MAX_EPOCHS,\n",
    "            'early_stopping_patience': EARLY_STOPPING_PATIENCE\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save experiment summary\n",
    "    summary_path = MODELS_DIR / 'experiment_summary.json'\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(experiment_summary, f, indent=2)\n",
    "    print(f\"✓ Experiment summary saved: {summary_path}\")\n",
    "    \n",
    "    # Generate human-readable summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EXPERIMENT SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(f\"Dataset: Edge-IIoTset\")\n",
    "    print(f\"  - {metadata['total_sequences']:,} sequences\")\n",
    "    print(f\"  - {metadata['sequence_length']} timesteps × {metadata['num_features']} features\")\n",
    "    print(f\"  - {metadata['num_classes']} classes: {', '.join(metadata['class_names'][:5])}{'...' if len(metadata['class_names']) > 5 else ''}\")\n",
    "    \n",
    "    print(f\"\\nModel: DeepFed Architecture\")\n",
    "    print(f\"  - Parameters: {model.count_params():,}\")\n",
    "    print(f\"  - GRU units: {GRU_UNITS}\")\n",
    "    print(f\"  - CNN filters: {CNN_FILTERS}\")\n",
    "    print(f\"  - MLP units: {MLP_UNITS}\")\n",
    "    \n",
    "    print(f\"\\nTraining Results:\")\n",
    "    print(f\"  - Best validation accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
    "    print(f\"  - Test accuracy: {results['test_accuracy']:.4f}\")\n",
    "    print(f\"  - Test loss: {results['test_loss']:.4f}\")\n",
    "    print(f\"  - Macro F1-score: {results['classification_report']['macro avg']['f1-score']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nArtifacts saved to:\")\n",
    "    print(f\"  - Models: {MODELS_DIR}\")\n",
    "    print(f\"  - Cache: {CACHE_DIR}\")\n",
    "    print(f\"  - Visualizations: {VISUALIZATIONS_DIR}\")\n",
    "    \n",
    "    # Performance analysis\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PERFORMANCE ANALYSIS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    test_acc = results['test_accuracy']\n",
    "    if test_acc > 0.9:\n",
    "        print(\"🎉 EXCELLENT performance! Model achieved >90% test accuracy\")\n",
    "    elif test_acc > 0.8:\n",
    "        print(\"✅ GOOD performance! Model achieved >80% test accuracy\")\n",
    "    elif test_acc > 0.7:\n",
    "        print(\"⚠️  MODERATE performance. Consider hyperparameter tuning or more data\")\n",
    "    else:\n",
    "        print(\"❌ POOR performance. Significant improvements needed\")\n",
    "    \n",
    "    # Class imbalance check\n",
    "    class_counts = np.bincount(y_test)\n",
    "    imbalance_ratio = class_counts.max() / class_counts.min()\n",
    "    if imbalance_ratio > 5:\n",
    "        print(f\"⚠️  High class imbalance detected ({imbalance_ratio:.1f}x). Consider advanced balancing techniques\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"RECOMMENDATIONS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    if test_acc < 0.8:\n",
    "        recommendations.append(\"- Increase model capacity (more units/filters)\")\n",
    "        recommendations.append(\"- Try different architectures or ensemble methods\")\n",
    "        recommendations.append(\"- Implement data augmentation for time series\")\n",
    "    \n",
    "    if imbalance_ratio > 3:\n",
    "        recommendations.append(\"- Use focal loss or class-weighted training\")\n",
    "        recommendations.append(\"- Implement SMOTE or other oversampling techniques\")\n",
    "        recommendations.append(\"- Collect more data for minority classes\")\n",
    "    \n",
    "    if len(history.history['loss']) >= MAX_EPOCHS:\n",
    "        recommendations.append(\"- Training stopped due to max epochs. Try increasing MAX_EPOCHS\")\n",
    "    \n",
    "    if not recommendations:\n",
    "        recommendations.append(\"- Model performance is good! Consider deployment\")\n",
    "        recommendations.append(\"- Experiment with different sequence lengths\")\n",
    "        recommendations.append(\"- Try transfer learning with similar datasets\")\n",
    "    \n",
    "    for rec in recommendations:\n",
    "        print(rec)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"NEXT STEPS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"1. Review visualizations in the 'visualizations/' directory\")\n",
    "    print(\"2. Analyze confusion matrix for misclassification patterns\")\n",
    "    print(\"3. Consider hyperparameter optimization (grid/random search)\")\n",
    "    print(\"4. Experiment with different architectures or preprocessing\")\n",
    "    print(\"5. Deploy model for inference or integrate into production pipeline\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EXPERIMENT COMPLETE ✓\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    return experiment_summary\n",
    "\n",
    "# Save artifacts and generate summary\n",
    "experiment_summary = save_artifacts_and_summary(results, metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# personalized FL:
# see: https://github.com/CharlieDinh/pFedMe

fl_opt:
  rounds: 500
  num_clients: 30
  frac: 1 # the fraction of clients in each FL round
  local_ep: 20 # local epoch
  local_bs: 64 # local batch size
  aggregation: fedavg # fedavg, fedavgm, fedbn, fedprox
  balanced_loader: false
  feat_aug: false

  # previously tried but not used
  # to enable, should first set /fl_opt/aggregation == fedavg_fs
  backbone_only: false
  imprint: false
  spread_out: false
  crt: false

criterions:
  # LwF Loss
  # def_file: ./loss/LwFLoss.py
  # loss_params: {Temp: 2, lamda: 1, loss_cls: ce, loss_kd: ce, num_classes: 10}    # lambda=0 does not means CE Loss. Just logits of certain classes are musked.
  # sce means smooth cross entropy, do not improve

  # CE Loss
  def_file: ./loss/KDLoss.py
  loss_params: { Temp: 2, lamda: 0, loss_cls: ce, loss_kd: kl } # lambda=0 means comman CE Loss

networks:
  feat_model:
    # CUB: ResNet34/18/10-512d, ResNet10h-256d, ResNet10s-192d; CIFAR: ResNet32/20/8-512/256/128d
    def_file: FC
    params:
      {
        dropout_rate: 0.1,
        fc_dims: [256, 128, 256],
        stage1_weights: false,
        use_fc: false,
        pretrain: false,
        l2_norm: false,
      }
    optim_params: { lr: 0.005, momentum: 0.9, weight_decay: 0.0001 }
    data_dim: 9
    feat_dim: 256
    fix: false
  classifier: # if l2_norm is true，bias should be false
    def_file: ./models/DotProductClassifier.py
    params: { num_classes: 8, l2_norm: false, bias: False, scale: 1 } 
    optim_params: { lr: 0.005, momentum: 0.9, weight_decay: 0.0001 } 
    fix: false

dataset:
  name: IMU
  # shot: 1000
  shot_few: 0 # if shot_few>0, (iid + non-iid) mixed mode; else, non-mixed mode
  num_classes: 8
  non_iidness: 1
  tao_ratio: 2
  imb_ratio: 0.01 # 0.1, 0.5, 1 for 10, 2, 1
  img_per_client_dist: uniform # (uniform, noraml, longtail, r_longtail）
  prefetch: false # load the whole dataset into RAM

metainfo:
  optimizer: adam
  lr_step: [300, 400, 450]
  display_step: 10
  display_grad: False
  display_grad_step: 10
  work_dir: ./exp_results
  exp_name: test
  log_dir: ./exp_results/test # log_dir = work_dir + exp_name

checkpoint:
  joint: checkpoints/alljoint_best_10.pth
